{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Jupyter Notebook for *Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis*\n",
    "\n",
    "### Code Author: Christopher J. Urban\n",
    "### Affil.: L. L. Thurstone Psychometric Laboratory in the Dept. of Psychology and Neuroscience, UNC-Chapel Hill\n",
    "### E-mail: cjurban@live.unc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook applies an importance-weighted  variational estimator (I-WAVE) for confirmatory multidimensional item response theory (MIRT) parameter estimation.\n",
    "\n",
    "First, I import packages and set display options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from scipy.linalg import block_diag\n",
    "import timeit\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "import subprocess\n",
    "from sklearn.inspection import permutation_importance\n",
    "import sys\n",
    "import os\n",
    "import gdown # for downloading from Google Drive\n",
    "from code.python.utils import *\n",
    "from code.python.helper_layers import *\n",
    "from code.python.base_class import *\n",
    "from code.python.mirt_vae import *\n",
    "from code.python.read_data import *\n",
    "from code.python.simulations import *\n",
    "from code.python.c2st import *\n",
    "from code.python.figures import *\n",
    "\n",
    "# Some display options.\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "np.set_printoptions(suppress = True)\n",
    "np.set_printoptions(threshold = sys.maxsize)\n",
    "\n",
    "# If CUDA is available, use it.\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\" : 1, \"pin_memory\" : True} if cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPIP-FFM Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download IPIP-FFM data and make data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm_url = \"https://drive.google.com/file/d/1XI_fOjja2BMOhUx6K7GKM9xOjNetZetf/view?usp=sharing\"\n",
    "ffm_path = \"data/ipip-ffm/\"; ffm_filename = \"ipip-ffm_recoded.csv\"\n",
    "Path(ffm_path).mkdir(parents = True, exist_ok = True)\n",
    "os.system(\"gdown --id 1XI_fOjja2BMOhUx6K7GKM9xOjNetZetf --output \\\"data/ipip-ffm/ipip-ffm_recoded.csv\\\"\")\n",
    "\n",
    "ffm_loader = torch.utils.data.DataLoader(\n",
    "    csv_dataset(csv_file = ffm_path + ffm_filename,\n",
    "                which_split = \"full\",\n",
    "                transform = to_tensor()),\n",
    "    batch_size = 32, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit five-factor model and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = \"results/ipip-ffm/five-factor/\"\n",
    "Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"run_time/\").mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "n_reps = 10\n",
    "\n",
    "for rep in range(n_reps):\n",
    "    # Set random seeds.\n",
    "    torch.manual_seed(rep)\n",
    "    np.random.seed(rep)\n",
    "\n",
    "    # Initialize model.\n",
    "    print(\"\\nStarting fitting for replication\", rep)\n",
    "    start = timeit.default_timer()\n",
    "    ffm_vae = MIRTVAEClass(input_dim = 250,\n",
    "                            inference_model_dims = [130],\n",
    "                            latent_dim = 5,\n",
    "                            n_cats = [5] * 50,\n",
    "                            learning_rate = 5e-3,\n",
    "                            device = device,\n",
    "                            Q = torch.from_numpy(block_diag(*[np.ones((10, 1))] * 5)).to(device).float(),\n",
    "                            correlated_factors = [0, 1, 2, 3, 4],\n",
    "                            steps_anneal = 1000)\n",
    "\n",
    "    # Fit model.\n",
    "    ffm_vae.run_training(ffm_loader, ffm_loader, iw_samples = 5)\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    print(\"Fitting completed in\", round(run_time, 2), \"seconds\")\n",
    "\n",
    "    # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "    loadings = ffm_vae.model.loadings.weight.data.numpy()\n",
    "    intercepts = ffm_vae.model.intercepts.bias.data.numpy()\n",
    "    scale_tril = ffm_vae.model.cholesky.weight().data.numpy()\n",
    "\n",
    "    # Compute approximate log-likelihood.\n",
    "    print(\"\\nComputing approx. LL for replication\", rep)\n",
    "    start = timeit.default_timer()\n",
    "    approx_ll = ffm_vae.bic(ffm_loader,\n",
    "                            iw_samples = 100)[1]\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Approx. LL computed in\", round(stop - start, 2), \"seconds\")\n",
    "    \n",
    "    # Save results.\n",
    "    np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "               loadings,\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "               intercepts,\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "               scale_tril,\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "               np.asarray([approx_ll]),\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"run_time/run_time_\" + str(rep) + \".txt\",\n",
    "               np.asarray([run_time]),\n",
    "               fmt = \"%f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain best fitting five-factor model and compute parameter estimate RMSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = \"results/ipip-ffm/five-factor/\"\n",
    "filenames = os.listdir(res_path + \"approx_ll/\")\n",
    "n_reps = len(filenames)\n",
    "\n",
    "# Read in fitted values.\n",
    "approx_ll_ls  = [np.loadtxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                 rep in range(n_reps)]\n",
    "ldgs_ls       = [np.loadtxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\", dtype = float) for\n",
    "                 rep in range(n_reps)]\n",
    "ints_ls       = [np.loadtxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\", dtype = float) for\n",
    "                 rep in range(n_reps)]\n",
    "scale_tril_ls = [np.loadtxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\", dtype = float) for\n",
    "                 rep in range(n_reps)]\n",
    "run_time_ls   = [np.loadtxt(res_path + \"run_time/run_time_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                 rep in range(n_reps)]\n",
    "\n",
    "# Obtain reference values.\n",
    "best_idx = approx_ll_ls.index(max(approx_ll_ls))\n",
    "ref_ldgs, ref_ints, ref_scale_tril = ldgs_ls.pop(best_idx), ints_ls.pop(best_idx), scale_tril_ls.pop(best_idx)\n",
    "ref_cor = np.matmul(ref_scale_tril, ref_scale_tril.T)\n",
    "\n",
    "# Calculate loadings RMSEs.\n",
    "ldgs_biases = [invert_factors(ldgs) - invert_factors(ref_ldgs) for ldgs in ldgs_ls]\n",
    "ldgs_rmses  = np.sqrt(reduce(np.add, [bias**2 for bias in ldgs_biases]) / len(ldgs_biases)).sum(axis = 1)\n",
    "\n",
    "# Calculate intercepts RMSEs.\n",
    "ints_biases = [ints - ref_ints for ints in ints_ls]\n",
    "ints_rmses  = np.sqrt(reduce(np.add, [bias**2 for bias in ints_biases]) / len(ints_biases))\n",
    "\n",
    "# Calculate factor correlation matrix RMSEs.\n",
    "cor_ls     = [np.matmul(scale_tril, scale_tril.T) for scale_tril in scale_tril_ls]\n",
    "cor_biases = [invert_cor(cor, ldgs) - invert_cor(ref_cor, ref_ldgs) for cor, ldgs in zip(cor_ls, ldgs_ls)]\n",
    "cor_rmses  = np.tril(np.sqrt(reduce(np.add, [bias**2 for bias in cor_biases]) / len(cor_biases)), k = -1)\n",
    "cor_rmses  = cor_rmses[np.nonzero(cor_rmses)]\n",
    "\n",
    "print(\"Mean Loadings RMSE = {:.3f} SD = {:.3f}\".format(np.mean(ldgs_rmses), np.std(ldgs_rmses)))\n",
    "print(\"Mean Intercepts RMSE = {:.3f} SD = {:.3f}\".format(np.mean(ints_rmses), np.std(ints_rmses)))\n",
    "print(\"Mean Factor Corr. RMSE = {:.3f} SD = {:.3f}\".format(np.mean(cor_rmses), np.std(cor_rmses)))\n",
    "print(\"Mean Run Time = {:.2f} SD = {:.2f}\".format(np.mean(run_time_ls), np.std(run_time_ls)))\n",
    "\n",
    "# Save parameter estimates for best-fitting model.\n",
    "save_path = \"data/simulations/gen_params/five-factor/\"\n",
    "Path(save_path).mkdir(parents = True, exist_ok = True)\n",
    "np.savetxt(save_path + \"gen_loadings.txt\",\n",
    "           ref_ldgs,\n",
    "           fmt = \"%.2f\")\n",
    "np.savetxt(save_path + \"gen_intercepts.txt\",\n",
    "           ref_ints,\n",
    "           fmt = \"%.2f\")\n",
    "np.savetxt(save_path + \"gen_scale_tril.txt\",\n",
    "           ref_scale_tril,\n",
    "           fmt = \"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct C2STs for five-factor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 10\n",
    "eps = 0.025\n",
    "n_cats = [5] * 50\n",
    "\n",
    "# Integer encode real data.\n",
    "real_data = ffm_loader.dataset.df.to_numpy()\n",
    "N = real_data.shape[0]\n",
    "idxs = np.concatenate((np.zeros(1), np.cumsum(n_cats)))\n",
    "ranges = [np.arange(int(l), int(u)) for l, u in zip(idxs, idxs[1:])]\n",
    "real_data_int = np.concatenate([np.expand_dims(np.argmax(real_data[:, rng], axis = 1), axis = 1) for\n",
    "                                rng in ranges], axis = 1)\n",
    "\n",
    "for rep in range(n_reps):\n",
    "    # List to store run times.\n",
    "    time_ls = []\n",
    "    \n",
    "    # Set random seeds.\n",
    "    torch.manual_seed(rep)\n",
    "    np.random.seed(rep)\n",
    "\n",
    "    # Load data generating parameters.\n",
    "    data_path = \"results/ipip-ffm/five-factor/\"\n",
    "    gen_loadings = torch.from_numpy(np.loadtxt(data_path + \"loadings/loadings_\" + str(rep) + \".txt\")).float()\n",
    "    gen_intercepts = torch.from_numpy(np.loadtxt(data_path + \"intercepts/intercepts_\" + str(rep) + \".txt\")).float()\n",
    "    gen_scale_tril = torch.from_numpy(np.loadtxt(data_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\")).float()\n",
    "\n",
    "    # Generate synthetic data.\n",
    "    synth_dist = dist.MultivariateNormal(loc = torch.zeros(5),\n",
    "                                         scale_tril = gen_scale_tril)\n",
    "    start = timeit.default_timer()\n",
    "    synth_data = sim_mirt(n_obs = N,\n",
    "                          distribution = synth_dist,\n",
    "                          loadings = gen_loadings,\n",
    "                          intercepts = gen_intercepts,\n",
    "                          n_cats = [5] * 50,\n",
    "                          dummy_code = False)[0]\n",
    "\n",
    "    # Create combined real and synthetic (from proposed model) data set.\n",
    "    X_prop = torch.cat([torch.from_numpy(real_data_int), synth_data], dim = 0).numpy()\n",
    "    y_prop = torch.cat([torch.ones(N), torch.zeros(N)]).numpy()\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Synthetic proposed model data created in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Conduct NN-based approximate C2ST for proposed model.\n",
    "    print(\"Fitting classifiers for proposed model\")\n",
    "    start = timeit.default_timer()\n",
    "    nn_prop_res = c2st(X_prop,\n",
    "                       y_prop,\n",
    "                       neural_network.MLPClassifier(max_iter = np.int(np.floor(10000 / (N / 200))),\n",
    "                                                    alpha = 0,\n",
    "                                                    random_state = rep),\n",
    "                       eps = eps,\n",
    "                       random_state = rep)\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"NN fitting completed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Conduct KNN-based approximate C2ST for proposed model.\n",
    "    start = timeit.default_timer()\n",
    "    _, X_prop_sub, _, y_prop_sub = train_test_split(X_prop, y_prop, test_size = 0.025)\n",
    "    knn_prop_res = c2st(X_prop_sub,\n",
    "                        y_prop_sub,\n",
    "                        neighbors.KNeighborsClassifier(n_neighbors = np.int(np.floor(np.sqrt(N))),\n",
    "                                                       metric = \"hamming\",\n",
    "                                                       algorithm = \"ball_tree\"),\n",
    "                        eps = eps,\n",
    "                        random_state = rep)\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"KNN fitting completed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Compute permutation importance for NN.\n",
    "    start = timeit.default_timer()\n",
    "    nn_imp = permutation_importance(nn_prop_res[\"clf\"],\n",
    "                                    nn_prop_res[\"X_test\"],\n",
    "                                    nn_prop_res[\"y_test\"], \n",
    "                                    scoring = \"accuracy\", \n",
    "                                    random_state = rep) \n",
    "    stop = timeit.default_timer()\n",
    "    print(\"NN importances computed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Compute permutation importance for KNN.\n",
    "    start = timeit.default_timer()\n",
    "    knn_imp = permutation_importance(knn_prop_res[\"clf\"],\n",
    "                                     knn_prop_res[\"X_test\"],\n",
    "                                     knn_prop_res[\"y_test\"], \n",
    "                                     scoring = \"accuracy\", \n",
    "                                     random_state = rep) \n",
    "    stop = timeit.default_timer()\n",
    "    print(\"KNN importances computed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Simulate synthetic data from baseline model.\n",
    "    start = timeit.default_timer()\n",
    "    base_data = sim_base(data = torch.from_numpy(real_data),\n",
    "                         n_cats = n_cats,\n",
    "                         dummy_code = False)\n",
    "\n",
    "    # Create combined real and synthetic (from baseline model) data set.\n",
    "    X_base = torch.cat([torch.from_numpy(real_data_int), base_data], dim = 0).numpy()\n",
    "    y_base = torch.cat([torch.ones(N), torch.zeros(N)]).numpy()\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Synthetic baseline model data created in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Conduct NN-based approximate C2ST for baseline model.\n",
    "    print(\"Fitting classifiers for baseline model\")\n",
    "    start = timeit.default_timer()\n",
    "    nn_acc_base = c2st(X_base,\n",
    "                       y_base,\n",
    "                       neural_network.MLPClassifier(max_iter = np.int(np.floor(10000 / (N / 200))),\n",
    "                                                    alpha = 0,\n",
    "                                                    random_state = rep),\n",
    "                       eps = eps,\n",
    "                       random_state = rep)[\"acc\"]\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"NN fitting completed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Conduct KNN-based approximate C2ST for baseline model.\n",
    "    start = timeit.default_timer()\n",
    "    _, X_base_sub, _, y_base_sub = train_test_split(X_base, y_base, test_size = 0.025)\n",
    "    knn_acc_base = c2st(X_base_sub,\n",
    "                        y_base_sub,\n",
    "                        neighbors.KNeighborsClassifier(n_neighbors = np.int(np.floor(np.sqrt(N))),\n",
    "                                                       metric = \"hamming\",\n",
    "                                                       algorithm = \"ball_tree\"),\n",
    "                        eps = eps,\n",
    "                        random_state = rep)[\"acc\"]\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"KNN fitting completed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Save results.\n",
    "    res_path = \"results/ipip-ffm/five-factor/\"\n",
    "    Path(res_path + \"c2st_run_times/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"c2st_run_times/c2st_run_times_\" + str(rep) + \".txt\",\n",
    "               np.asarray(time_ls),\n",
    "               fmt = \"%f\")\n",
    "    Path(res_path + \"nn_prop_res/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(nn_prop_res, res_path + \"nn_prop_res/nn_prop_res_\" + str(rep))\n",
    "    Path(res_path + \"knn_prop_res/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(knn_prop_res, res_path + \"knn_prop_res/knn_prop_res_\" + str(rep))\n",
    "    Path(res_path + \"nn_imp/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(nn_imp, res_path + \"nn_imp/nn_imp_\" + str(rep))\n",
    "    Path(res_path + \"knn_imp/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(knn_imp, res_path + \"knn_imp/knn_imp_\" + str(rep))\n",
    "    Path(res_path + \"nn_acc_base/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"nn_acc_base/nn_acc_base_\" + str(rep) + \".txt\",\n",
    "               np.asarray([nn_acc_base]),\n",
    "               fmt = \"%f\")\n",
    "    Path(res_path + \"knn_acc_base/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"knn_acc_base/knn_acc_base_\" + str(rep) + \".txt\",\n",
    "               np.asarray([knn_acc_base]),\n",
    "               fmt = \"%f\")\n",
    "\n",
    "# Load results.\n",
    "res_path = \"results/ipip-ffm/five-factor/\"\n",
    "time_ls = [np.loadtxt(res_path + \"c2st_run_times/c2st_run_times_\" + str(rep) + \".txt\") for rep in range(n_reps)]\n",
    "nn_acc_prop_ls = [load_obj(res_path + \"nn_prop_res/nn_prop_res_\" + str(rep))[\"acc\"] for rep in range(n_reps)]\n",
    "nn_p_val_ls = [load_obj(res_path + \"nn_prop_res/nn_prop_res_\" + str(rep))[\"p_val\"] for rep in range(n_reps)]\n",
    "nn_acc_base_ls = [np.loadtxt(res_path + \"nn_acc_base/nn_acc_base_\" + str(rep) + \".txt\").item() for\n",
    "                  rep in range(n_reps)]\n",
    "nn_imp_ls = [load_obj(res_path + \"nn_imp/nn_imp_\" + str(rep)) for rep in range(n_reps)]\n",
    "knn_acc_prop_ls = [load_obj(res_path + \"knn_prop_res/knn_prop_res_\" + str(rep))[\"acc\"] for rep in range(n_reps)]\n",
    "knn_p_val_ls = [load_obj(res_path + \"knn_prop_res/knn_prop_res_\" + str(rep))[\"p_val\"] for rep in range(n_reps)]\n",
    "knn_acc_base_ls = [np.loadtxt(res_path + \"knn_acc_base/knn_acc_base_\" + str(rep) + \".txt\").item() for\n",
    "                   rep in range(n_reps)]\n",
    "knn_imp_ls = [load_obj(res_path + \"knn_imp/knn_imp_\" + str(rep)) for rep in range(n_reps)]\n",
    "\n",
    "# Compute relative fit indices.\n",
    "M_prop = 265\n",
    "M_base = 200\n",
    "knn_rfi_ls = [c2st_rfi(acc_prop, acc_base, M_prop, M_base, lambda a : a) for\n",
    "              acc_prop, acc_base in zip(knn_acc_prop_ls, knn_acc_base_ls)]\n",
    "nn_rfi_ls = [c2st_rfi(acc_prop, acc_base, M_prop, M_base, lambda a : a) for\n",
    "             acc_prop, acc_base in zip(nn_acc_prop_ls, nn_acc_base_ls)]\n",
    "\n",
    "print((\"Classifier two-sample tests completed\"\n",
    "       \"\\nMean NN accuracy =\"), np.mean(nn_acc_prop_ls), \"SD =\", np.std(nn_acc_prop_ls),\n",
    "       \"\\np-values =\", nn_p_val_ls,\n",
    "       \"\\nMean KNN accuracy =\", np.mean(knn_acc_prop_ls), \"SD =\", np.std(knn_acc_prop_ls),\n",
    "       \"\\np-values =\", knn_p_val_ls,\n",
    "       \"\\nMean NN base model accuracy = \", np.mean(nn_acc_base_ls), \"SD =\", np.std(nn_acc_base_ls),\n",
    "       \"\\nMean KNN base model accuracy = \", np.mean(knn_acc_base_ls), \"SD =\", np.std(knn_acc_base_ls),\n",
    "       \"\\nMean NN-RFI =\", np.mean(nn_rfi_ls), \"SD = \", np.std(nn_rfi_ls),\n",
    "       \"\\nMean KNN-RFI =\", np.mean(knn_rfi_ls), \"SD = \", np.std(knn_rfi_ls),\n",
    "       \"\\nMean run times =\", np.mean(time_ls, axis = 0), \"SDs = \", np.std(time_ls, axis = 0))\n",
    "\n",
    "# Create and save permutation importances figure.\n",
    "fig_path = \"figures/\"\n",
    "Path(fig_path).mkdir(parents = True, exist_ok = True)\n",
    "fig = importance_plot(knn_imp_ls,\n",
    "                      nn_imp_ls,\n",
    "                      varnames = [\"EXT\\n(Items 1–10)\",\n",
    "                                  \"EST\\n(Items 11–20)\",\n",
    "                                  \"AGR\\n(Items 21–30)\",\n",
    "                                  \"CSN\\n(Items 31–40)\",\n",
    "                                  \"OPN\\n(Items 41–50)\"],\n",
    "                      knn_title = \"KNN Classifiers\",\n",
    "                      nn_title = \"NN Classifiers\",\n",
    "                      knn_ylim = [-0.0025, 0.0125],\n",
    "                      nn_ylim = [-0.0025, 0.0525],\n",
    "                      hatch_list = [16, 17, 19, 40, 47])\n",
    "fig.show()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"importances_five-factor_ipip-ffm.pdf\")\n",
    "pdf.savefig(fig, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit seven-factor model and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = \"results/ipip-ffm/seven-factor/\"\n",
    "Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "Path(res_path + \"run_time/\").mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Make linear constraints matrix.\n",
    "A = torch.from_numpy(block_diag(*[np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.zeros([50, 50])])).to(device).float()\n",
    "A[266, 266] += 1; A[267, 266] += 1; A[340, 340] += 1; A[347, 340] += 1\n",
    "\n",
    "n_reps = 10\n",
    "\n",
    "for rep in range(n_reps):\n",
    "    # Set random seeds.\n",
    "    torch.manual_seed(rep)\n",
    "    np.random.seed(rep)\n",
    "\n",
    "    # Initialize model.\n",
    "    print(\"\\nStarting fitting for replication\", rep)\n",
    "    start = timeit.default_timer()\n",
    "    ffm_vae = MIRTVAEClass(input_dim = 250,\n",
    "                           inference_model_dims = [130],\n",
    "                           latent_dim = 7,\n",
    "                           n_cats = [5] * 50,\n",
    "                           learning_rate = 5e-3,\n",
    "                           device = device,\n",
    "                           A = A,\n",
    "                           correlated_factors = [0, 1, 2, 3, 4],\n",
    "                           steps_anneal = 1000)\n",
    "\n",
    "    # Fit model.\n",
    "    ffm_vae.run_training(ffm_loader, ffm_loader, iw_samples = 5)\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "    print(\"Fitting completed in\", round(run_time, 2), \"seconds\")\n",
    "\n",
    "    # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "    loadings = ffm_vae.model.loadings.weight().data.numpy()\n",
    "    intercepts = ffm_vae.model.intercepts.bias.data.numpy()\n",
    "    scale_tril = ffm_vae.model.cholesky.weight().data.numpy()\n",
    "\n",
    "    # Compute approximate log-likelihood.\n",
    "    print(\"\\nComputing approx. LL for replication\", rep)\n",
    "    start = timeit.default_timer()\n",
    "    approx_ll = ffm_vae.bic(ffm_loader,\n",
    "                            iw_samples = 100)[1]\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Approx. LL computed in\", round(stop - start, 2), \"seconds\")\n",
    "    \n",
    "    # Save results.\n",
    "    np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "               loadings,\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "               intercepts,\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "               scale_tril,\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "               np.asarray([approx_ll]),\n",
    "               fmt = \"%f\")\n",
    "    np.savetxt(res_path + \"run_time/run_time_\" + str(rep) + \".txt\",\n",
    "               np.asarray([run_time]),\n",
    "               fmt = \"%f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain best fitting seven-factor model and compute parameter estimate RMSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = \"results/ipip-ffm/seven-factor/\"\n",
    "filenames = os.listdir(res_path + \"approx_ll/\")\n",
    "n_reps = len(filenames)\n",
    "\n",
    "# Read in fitted values.\n",
    "approx_ll_ls  = [np.loadtxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                 rep in range(n_reps)]\n",
    "ldgs_ls       = [np.loadtxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\", dtype = float) for\n",
    "                 rep in range(n_reps)]\n",
    "ints_ls       = [np.loadtxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\", dtype = float) for\n",
    "                 rep in range(n_reps)]\n",
    "scale_tril_ls = [np.loadtxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\", dtype = float) for\n",
    "                 rep in range(n_reps)]\n",
    "run_time_ls   = [np.loadtxt(res_path + \"run_time/run_time_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                 rep in range(n_reps)]\n",
    "\n",
    "# Obtain reference values.\n",
    "best_idx = approx_ll_ls.index(max(approx_ll_ls))\n",
    "ref_ldgs, ref_ints, ref_scale_tril = ldgs_ls.pop(best_idx), ints_ls.pop(best_idx), scale_tril_ls.pop(best_idx)\n",
    "ref_cor = np.matmul(ref_scale_tril, ref_scale_tril.T)\n",
    "\n",
    "# Calculate loadings RMSEs.\n",
    "ldgs_biases = [invert_factors(ldgs) - invert_factors(ref_ldgs) for ldgs in ldgs_ls]\n",
    "ldgs_rmses  = np.sqrt(reduce(np.add, [bias**2 for bias in ldgs_biases]) / len(ldgs_biases))[ref_ldgs.nonzero()]\n",
    "\n",
    "# Calculate intercepts RMSEs.\n",
    "ints_biases = [ints - ref_ints for ints in ints_ls]\n",
    "ints_rmses  = np.sqrt(reduce(np.add, [bias**2 for bias in ints_biases]) / len(ints_biases))\n",
    "\n",
    "# Calculate factor correlation matrix RMSEs.\n",
    "cor_ls     = [np.matmul(scale_tril, scale_tril.T) for scale_tril in scale_tril_ls]\n",
    "cor_biases = [invert_cor(cor, ldgs) - invert_cor(ref_cor, ref_ldgs) for cor, ldgs in zip(cor_ls, ldgs_ls)]\n",
    "cor_rmses  = np.tril(np.sqrt(reduce(np.add, [bias**2 for bias in cor_biases]) / len(cor_biases)), k = -1)\n",
    "cor_rmses  = cor_rmses[np.nonzero(cor_rmses)]\n",
    "\n",
    "print(\"Mean Loadings RMSE = {:.3f} SD = {:.3f}\".format(np.mean(ldgs_rmses), np.std(ldgs_rmses)))\n",
    "print(\"Mean Intercepts RMSE = {:.3f} SD = {:.3f}\".format(np.mean(ints_rmses), np.std(ints_rmses)))\n",
    "print(\"Mean Factor Corr. RMSE = {:.3f} SD = {:.3f}\".format(np.mean(cor_rmses), np.std(cor_rmses)))\n",
    "print(\"Mean Run Time = {:.2f} SD = {:.2f}\".format(np.mean(run_time_ls), np.std(run_time_ls)))\n",
    "\n",
    "# Save parameter estimates for best-fitting model.\n",
    "save_path = \"data/simulations/gen_params/seven-factor/\"\n",
    "Path(save_path).mkdir(parents = True, exist_ok = True)\n",
    "np.savetxt(save_path + \"gen_loadings.txt\",\n",
    "           ref_ldgs,\n",
    "           fmt = \"%.2f\")\n",
    "np.savetxt(save_path + \"gen_intercepts.txt\",\n",
    "           ref_ints,\n",
    "           fmt = \"%.2f\")\n",
    "np.savetxt(save_path + \"gen_scale_tril.txt\",\n",
    "           ref_scale_tril,\n",
    "           fmt = \"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct C2STs for seven-factor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 10\n",
    "eps = 0.025\n",
    "n_cats = [5] * 50\n",
    "\n",
    "# Integer encode real data.\n",
    "real_data = ffm_loader.dataset.df.to_numpy()\n",
    "N = real_data.shape[0]\n",
    "idxs = np.concatenate((np.zeros(1), np.cumsum(n_cats)))\n",
    "ranges = [np.arange(int(l), int(u)) for l, u in zip(idxs, idxs[1:])]\n",
    "real_data_int = np.concatenate([np.expand_dims(np.argmax(real_data[:, rng], axis = 1), axis = 1) for\n",
    "                                rng in ranges], axis = 1)\n",
    "\n",
    "for rep in range(n_reps):\n",
    "    # List to store run times.\n",
    "    time_ls = []\n",
    "    \n",
    "    # Set random seeds.\n",
    "    torch.manual_seed(rep)\n",
    "    np.random.seed(rep)\n",
    "\n",
    "    # Load data generating parameters.\n",
    "    data_path = \"results/ipip-ffm/seven-factor/\"\n",
    "    gen_loadings = torch.from_numpy(np.loadtxt(data_path + \"loadings/loadings_\" + str(rep) + \".txt\")).float()\n",
    "    gen_intercepts = torch.from_numpy(np.loadtxt(data_path + \"intercepts/intercepts_\" + str(rep) + \".txt\")).float()\n",
    "    gen_scale_tril = torch.from_numpy(np.loadtxt(data_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\")).float()\n",
    "\n",
    "    # Generate synthetic data.\n",
    "    synth_dist = dist.MultivariateNormal(loc = torch.zeros(7),\n",
    "                                         scale_tril = gen_scale_tril)\n",
    "    start = timeit.default_timer()\n",
    "    synth_data = sim_mirt(n_obs = N,\n",
    "                          distribution = synth_dist,\n",
    "                          loadings = gen_loadings,\n",
    "                          intercepts = gen_intercepts,\n",
    "                          n_cats = [5] * 50,\n",
    "                          dummy_code = False)[0]\n",
    "\n",
    "    # Create combined real and synthetic (from proposed model) data set.\n",
    "    X_prop = torch.cat([torch.from_numpy(real_data_int), synth_data], dim = 0).numpy()\n",
    "    y_prop = torch.cat([torch.ones(N), torch.zeros(N)]).numpy()\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Synthetic proposed model data created in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Conduct NN-based approximate C2ST for proposed model.\n",
    "    print(\"Fitting classifiers for proposed model\")\n",
    "    start = timeit.default_timer()\n",
    "    nn_prop_res = c2st(X_prop,\n",
    "                       y_prop,\n",
    "                       neural_network.MLPClassifier(max_iter = np.int(np.floor(10000 / (N / 200))),\n",
    "                                                    alpha = 0,\n",
    "                                                    random_state = rep),\n",
    "                       eps = eps,\n",
    "                       random_state = rep)\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"NN fitting completed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Conduct KNN-based approximate C2ST for proposed model.\n",
    "    start = timeit.default_timer()\n",
    "    _, X_prop_sub, _, y_prop_sub = train_test_split(X_prop, y_prop, test_size = 0.025)\n",
    "    knn_prop_res = c2st(X_prop_sub,\n",
    "                        y_prop_sub,\n",
    "                        neighbors.KNeighborsClassifier(n_neighbors = np.int(np.floor(np.sqrt(N))),\n",
    "                                                       metric = \"hamming\",\n",
    "                                                       algorithm = \"ball_tree\"),\n",
    "                        eps = eps,\n",
    "                        random_state = rep)\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"KNN fitting completed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Compute permutation importance for NN.\n",
    "    start = timeit.default_timer()\n",
    "    nn_imp = permutation_importance(nn_prop_res[\"clf\"],\n",
    "                                    nn_prop_res[\"X_test\"],\n",
    "                                    nn_prop_res[\"y_test\"], \n",
    "                                    scoring = \"accuracy\", \n",
    "                                    random_state = rep) \n",
    "    stop = timeit.default_timer()\n",
    "    print(\"NN importances computed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Compute permutation importance for KNN.\n",
    "    start = timeit.default_timer()\n",
    "    knn_imp = permutation_importance(knn_prop_res[\"clf\"],\n",
    "                                     knn_prop_res[\"X_test\"],\n",
    "                                     knn_prop_res[\"y_test\"], \n",
    "                                     scoring = \"accuracy\", \n",
    "                                     random_state = rep) \n",
    "    stop = timeit.default_timer()\n",
    "    print(\"KNN importances computed in\", round(stop - start, 2), \"seconds\")\n",
    "    time_ls.append(stop - start)\n",
    "\n",
    "    # Save results.\n",
    "    res_path = \"results/ipip-ffm/seven-factor/\"\n",
    "    Path(res_path + \"c2st_run_times/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"c2st_run_times/c2st_run_times_\" + str(rep) + \".txt\",\n",
    "               np.asarray(time_ls),\n",
    "               fmt = \"%f\")\n",
    "    Path(res_path + \"nn_prop_res/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(nn_prop_res, res_path + \"nn_prop_res/nn_prop_res_\" + str(rep))\n",
    "    Path(res_path + \"knn_prop_res/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(knn_prop_res, res_path + \"knn_prop_res/knn_prop_res_\" + str(rep))\n",
    "    Path(res_path + \"nn_imp/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(nn_imp, res_path + \"nn_imp/nn_imp_\" + str(rep))\n",
    "    Path(res_path + \"knn_imp/\").mkdir(parents = True, exist_ok = True)\n",
    "    save_obj(knn_imp, res_path + \"knn_imp/knn_imp_\" + str(rep))\n",
    "\n",
    "# Load results.\n",
    "res_path = \"results/ipip-ffm/seven-factor/\"\n",
    "time_ls = [np.loadtxt(res_path + \"c2st_run_times/c2st_run_times_\" + str(rep) + \".txt\") for rep in range(n_reps)]\n",
    "nn_acc_prop_ls = [load_obj(res_path + \"nn_prop_res/nn_prop_res_\" + str(rep))[\"acc\"] for rep in range(n_reps)]\n",
    "nn_p_val_ls = [load_obj(res_path + \"nn_prop_res/nn_prop_res_\" + str(rep))[\"p_val\"] for rep in range(n_reps)]\n",
    "nn_acc_base_ls = [np.loadtxt(\"results/ipip-ffm/five-factor/nn_acc_base/nn_acc_base_\" + str(rep) + \".txt\").item() for\n",
    "                  rep in range(n_reps)]\n",
    "nn_imp_ls = [load_obj(res_path + \"nn_imp/nn_imp_\" + str(rep)) for rep in range(n_reps)]\n",
    "knn_acc_prop_ls = [load_obj(res_path + \"knn_prop_res/knn_prop_res_\" + str(rep))[\"acc\"] for rep in range(n_reps)]\n",
    "knn_p_val_ls = [load_obj(res_path + \"knn_prop_res/knn_prop_res_\" + str(rep))[\"p_val\"] for rep in range(n_reps)]\n",
    "knn_acc_base_ls = [np.loadtxt(\"results/ipip-ffm/five-factor/knn_acc_base/knn_acc_base_\" + str(rep) + \".txt\").item() for\n",
    "                   rep in range(n_reps)]\n",
    "knn_imp_ls = [load_obj(res_path + \"knn_imp/knn_imp_\" + str(rep)) for rep in range(n_reps)]\n",
    "\n",
    "# Compute relative fit indices.\n",
    "M_prop = 265\n",
    "M_base = 200\n",
    "knn_rfi_ls = [c2st_rfi(acc_prop, acc_base, M_prop, M_base, lambda a : a) for\n",
    "              acc_prop, acc_base in zip(knn_acc_prop_ls, knn_acc_base_ls)]\n",
    "nn_rfi_ls = [c2st_rfi(acc_prop, acc_base, M_prop, M_base, lambda a : a) for\n",
    "             acc_prop, acc_base in zip(nn_acc_prop_ls, nn_acc_base_ls)]\n",
    "\n",
    "print((\"Classifier two-sample tests completed\"\n",
    "       \"\\nMean NN accuracy =\"), np.mean(nn_acc_prop_ls), \"SD =\", np.std(nn_acc_prop_ls),\n",
    "      \"\\np-values =\", nn_p_val_ls,\n",
    "      \"\\nMean KNN accuracy =\", np.mean(knn_acc_prop_ls), \"SD =\", np.std(knn_acc_prop_ls),\n",
    "      \"\\np-values =\", knn_p_val_ls,\n",
    "      \"\\nMean NN base model accuracy = \", np.mean(nn_acc_base_ls), \"SD =\", np.std(nn_acc_base_ls),\n",
    "      \"\\nMean KNN base model accuracy = \", np.mean(knn_acc_base_ls), \"SD =\", np.std(knn_acc_base_ls),\n",
    "      \"\\nMean NN-RFI =\", np.mean(nn_rfi_ls), \"SD = \", np.std(nn_rfi_ls),\n",
    "      \"\\nMean KNN-RFI =\", np.mean(knn_rfi_ls), \"SD = \", np.std(knn_rfi_ls),\n",
    "      \"\\nMean run times =\", np.mean(time_ls, axis = 0), \"SDs = \", np.std(time_ls, axis = 0))\n",
    "\n",
    "# Create and save permutation importances figure.\n",
    "fig_path = \"figures/\"\n",
    "Path(fig_path).mkdir(parents = True, exist_ok = True)\n",
    "fig = importance_plot(knn_imp_ls,\n",
    "                      nn_imp_ls,\n",
    "                      varnames = [\"EXT\\n(Items 1–10)\",\n",
    "                                  \"EST\\n(Items 11–20)\",\n",
    "                                  \"AGR\\n(Items 21–30)\",\n",
    "                                  \"CSN\\n(Items 31–40)\",\n",
    "                                  \"OPN\\n(Items 41–50)\"],\n",
    "                      knn_title = \"KNN Classifiers\",\n",
    "                      nn_title = \"NN Classifiers\",\n",
    "                      knn_ylim = [-0.0025, 0.0125],\n",
    "                      nn_ylim = [-0.0025, 0.0525],\n",
    "                      hatch_list = [16, 17, 19, 40, 47])\n",
    "fig.show()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"importances_seven-factor_ipip-ffm.pdf\")\n",
    "pdf.savefig(fig, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance-Weighting Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate MIRT data for conducting importance-weighting simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "data_path = \"data/simulations/\"\n",
    "gen_loadings = torch.from_numpy(np.loadtxt(data_path + \"gen_params/five-factor/gen_loadings.txt\")).float()\n",
    "gen_intercepts = torch.from_numpy(np.loadtxt(data_path + \"gen_params/five-factor/gen_intercepts.txt\")).float()\n",
    "gen_scale_tril = torch.from_numpy(np.loadtxt(data_path + \"gen_params/five-factor/gen_scale_tril.txt\")).float()\n",
    "\n",
    "# Set some values for simulating data.\n",
    "sample_size_ls = [500, 2500, 12500, 62500]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "\n",
    "# Simulate and save data sets.\n",
    "for N_idx, N in enumerate(sample_size_ls):  \n",
    "    for rep in range(n_reps):\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep)\n",
    "        np.random.seed(rep)\n",
    "\n",
    "        # Simulate data.\n",
    "        sim_dist = dist.MultivariateNormal(loc = torch.zeros(5),\n",
    "                                           scale_tril = gen_scale_tril)\n",
    "        sim_data = sim_mirt(n_obs = N,\n",
    "                            distribution = sim_dist,\n",
    "                            loadings = gen_loadings,\n",
    "                            intercepts = gen_intercepts,\n",
    "                            n_cats = n_cats,\n",
    "                            efficient = False)[0].numpy()\n",
    "        \n",
    "        # Save data set.\n",
    "        cell_path = data_path + \"importance-weighting/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(cell_path).mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(cell_path + \"data_\" + str(rep) + \".gz\",\n",
    "                   sim_data,\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "print(\"Finished simulating data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw_samples_ls = [1, 5, 25]\n",
    "sample_size_ls = [500, 2500, 12500, 62500]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "\n",
    "# Loop through simulation cells and replications.\n",
    "for iw_samples_idx, iw_samples in enumerate(iw_samples_ls):\n",
    "    for N_idx, N in enumerate(sample_size_ls):\n",
    "        print(\"\\nStarting replications for N =\", N, \"IW Samples =\", iw_samples)\n",
    "\n",
    "        for rep in range(n_reps):\n",
    "            print(\"\\nStarting fitting for replication\", rep)\n",
    "\n",
    "            # Load data.\n",
    "            data_path = \"data/simulations/importance-weighting/\"\n",
    "            cell_path = data_path + \"sim_cell_\" + str(N_idx) + \"/\"\n",
    "            data = np.loadtxt(cell_path + \"data_\" + str(rep) + \".gz\")\n",
    "\n",
    "            # Make data loader.\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                tensor_dataset(torch.from_numpy(data)),\n",
    "                batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "            # Set random seeds.\n",
    "            torch.manual_seed(rep * 100)\n",
    "            np.random.seed(rep * 100)\n",
    "\n",
    "            # Initialize model.\n",
    "            start = timeit.default_timer()\n",
    "            vae = MIRTVAEClass(input_dim = 250,\n",
    "                               inference_model_dims = [130],\n",
    "                               latent_dim = 5,\n",
    "                               n_cats = n_cats,\n",
    "                               learning_rate = 5e-3,\n",
    "                               device = device,\n",
    "                               Q = torch.from_numpy(block_diag(*([np.ones((10, 1))] * 5))).to(device).float(),\n",
    "                               correlated_factors = [0, 1, 2, 3, 4],\n",
    "                               steps_anneal = 1000)\n",
    "\n",
    "            # Fit model.\n",
    "            vae.run_training(data_loader, data_loader, iw_samples = iw_samples)\n",
    "            stop = timeit.default_timer()\n",
    "            run_time = stop - start\n",
    "            print(\"Fitting completed in\", round(run_time, 2), \"seconds\")\n",
    "\n",
    "            # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "            loadings = vae.model.loadings.weight.data\n",
    "            intercepts = vae.model.intercepts.bias.data\n",
    "            scale_tril = vae.model.cholesky.weight().data\n",
    "\n",
    "            # Compute approximate log-likelihood.\n",
    "            print(\"Computing approx. LL for replication\", rep)\n",
    "            start = timeit.default_timer()\n",
    "            approx_ll = vae.bic(data_loader,\n",
    "                                    iw_samples = 100)[1]\n",
    "            stop = timeit.default_timer()\n",
    "            print(\"Approx. LL computed in\", round(stop - start, 2), \"seconds\")\n",
    "\n",
    "            # Make simulation cell directory.\n",
    "            res_path = (\"results/simulations/importance-weighting/sim_cell_\" + str(iw_samples_idx) +\n",
    "                                    \"_\" + str(N_idx) + \"/\")\n",
    "            Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "            # Save extracted results.\n",
    "            Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                       loadings.numpy(),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "                       intercepts.numpy(),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "                       scale_tril.numpy(),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([approx_ll]),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"run_time/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"run_time/run_time_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([run_time]),\n",
    "                       fmt = \"%f\")\n",
    "            \n",
    "\"\"\"\n",
    "Needed to manually refit the following runs due to convergence to poor local minima:\n",
    "    IW Samples = 5:\n",
    "        N = 500: 1, 14, 22, [26], 36, 59, 88\n",
    "        N = 2500: 56\n",
    "    IW Samples = 25:\n",
    "        N = 500: [15], 42, 54, 55, 73, 76, 78, 84, 91, [93]\n",
    "        N = 2500: 87\n",
    "        N = 12500: 50\n",
    "        N = 62500: 10\n",
    "Bad runs were identified via their outlying approx. LLs.\n",
    "Used seed = rep * 1000, then seed = rep * 2000 for runs in brackets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bias, MSE, and fitting time plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "data_path = \"data/simulations/\"\n",
    "gen_loadings = np.loadtxt(data_path + \"gen_params/five-factor/gen_loadings.txt\")\n",
    "gen_intercepts = np.loadtxt(data_path + \"gen_params/five-factor/gen_intercepts.txt\")\n",
    "gen_scale_tril = np.loadtxt(data_path + \"gen_params/five-factor/gen_scale_tril.txt\")\n",
    "\n",
    "iw_samples_ls = [1, 5, 25]\n",
    "sample_size_ls = [500, 2500, 12500, 62500]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "\n",
    "# Make list to store I-WAVE results.\n",
    "res_path = \"results/simulations/importance-weighting/\"\n",
    "iwave_cell_res_ls = []\n",
    "\n",
    "# Read in I-WAVE results.\n",
    "for iw_samples_idx in range(len(iw_samples_ls)):\n",
    "    for N_idx in range(len(sample_size_ls)):\n",
    "        keys = [\"approx_ll\", \"run_time\", \"loadings\", \"intercepts\", \"cor\"]\n",
    "        cell_res = {key : [] for key in keys}\n",
    "        sim_cell = str(iw_samples_idx) + \"_\" + str(N_idx)\n",
    "\n",
    "        # Read results.\n",
    "        cell_res[\"loadings\"] = [np.loadtxt(res_path + \"sim_cell_\" + sim_cell + \"/loadings/loadings_\" +\n",
    "                                            str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "        cell_res[\"intercepts\"] = [np.loadtxt(res_path + \"sim_cell_\" + sim_cell + \"/intercepts/intercepts_\" +\n",
    "                                             str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "        scale_tril_ls = [np.loadtxt(res_path + \"sim_cell_\" + sim_cell + \"/scale_tril/scale_tril_\" +\n",
    "                                    str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "        cell_res[\"cor\"] = [np.matmul(scale_tril, scale_tril.T) for scale_tril in scale_tril_ls]\n",
    "        cell_res[\"approx_ll\"] = [np.loadtxt(res_path + \"sim_cell_\" + sim_cell + \"/approx_ll/approx_ll_\" +\n",
    "                                            str(rep) + \".txt\", dtype = float).item() for rep in range(n_reps)]\n",
    "        cell_res[\"run_time\"] = [np.loadtxt(res_path + \"sim_cell_\" + sim_cell + \"/run_time/run_time_\" +\n",
    "                                           str(rep) + \".txt\", dtype = float).item() for rep in range(n_reps)]\n",
    "\n",
    "        iwave_cell_res_ls.append(cell_res)\n",
    "        \n",
    "bias = bias_boxplots(cell_res_ls = iwave_cell_res_ls,\n",
    "                     gen_cor = np.matmul(gen_scale_tril, gen_scale_tril.T),\n",
    "                     gen_loadings = gen_loadings,\n",
    "                     gen_intercepts = gen_intercepts,\n",
    "                     sample_size_ls = sample_size_ls,\n",
    "                     power = 1,\n",
    "                     ldgs_lims = [-.3, .15])\n",
    "bias.show()\n",
    "\n",
    "mse = bias_boxplots(cell_res_ls = iwave_cell_res_ls,\n",
    "                    gen_cor = np.matmul(gen_scale_tril, gen_scale_tril.T),\n",
    "                    gen_loadings = gen_loadings,\n",
    "                    gen_intercepts = gen_intercepts,\n",
    "                    sample_size_ls = sample_size_ls,\n",
    "                    power = 2)\n",
    "mse.show()\n",
    "\n",
    "times = time_plot(cell_res_ls = iwave_cell_res_ls,\n",
    "                  sample_size_ls = sample_size_ls,\n",
    "                  y_lims = [0, 300])\n",
    "times.show()\n",
    "\n",
    "# Save plots to PDFs.\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"figures/bias_plots_importance-weighting.pdf\")\n",
    "pdf.savefig(bias, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"figures/mse_plots_importance-weighting.pdf\")\n",
    "pdf.savefig(mse, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"figures/time_plot_importance-weighting.pdf\")\n",
    "pdf.savefig(times, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MH-RM Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate MIRT data for conducting MH-RM comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "data_path = \"data/simulations/\"\n",
    "gen_loadings = np.loadtxt(data_path + \"gen_params/five-factor/gen_loadings.txt\")\n",
    "gen_intercepts = np.loadtxt(data_path + \"gen_params/five-factor/gen_intercepts.txt\")\n",
    "gen_scale_tril = np.loadtxt(data_path + \"gen_params/five-factor/gen_scale_tril.txt\")\n",
    "\n",
    "# Modify data generating parameters.\n",
    "gen_loadings, gen_intercepts, gen_scale_tril = make_gen_params(orig_loadings = gen_loadings,\n",
    "                                                               orig_intercepts = gen_intercepts,\n",
    "                                                               orig_n_cats = 5,\n",
    "                                                               new_n_cats = 5,\n",
    "                                                               orig_cov = gen_scale_tril,\n",
    "                                                               factor_mul = 2)\n",
    "\n",
    "# Set some values for simulating data.\n",
    "sample_size_ls = [1000, 2500, 5000, 10000]\n",
    "n_cats = [5] * 100\n",
    "n_reps = 100\n",
    "\n",
    "# Simulate and save data sets.\n",
    "for N_idx, N in enumerate(sample_size_ls):  \n",
    "    for rep in range(n_reps):\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep)\n",
    "        np.random.seed(rep)\n",
    "\n",
    "        # Simulate data.\n",
    "        sim_dist = dist.MultivariateNormal(loc = torch.zeros(10),\n",
    "                                           scale_tril = gen_scale_tril)\n",
    "        sim_data = sim_mirt(n_obs = N,\n",
    "                            distribution = sim_dist,\n",
    "                            loadings = gen_loadings,\n",
    "                            intercepts = gen_intercepts,\n",
    "                            n_cats = n_cats,\n",
    "                            efficient = False)[0].numpy()\n",
    "        \n",
    "        # Save data set.\n",
    "        cell_path = data_path + \"mhrm/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(cell_path).mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(cell_path + \"data_\" + str(rep) + \".gz\",\n",
    "                   sim_data,\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "print(\"Finished simulating data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some values for model fitting.\n",
    "sample_size_ls = [1000, 2500, 5000, 10000]\n",
    "n_cats = [5] * 100\n",
    "n_reps = 100\n",
    "\n",
    "# Loop through simulation cells and replications.\n",
    "for N_idx, N in enumerate(sample_size_ls):\n",
    "    print(\"\\nStarting replications for N =\", N)\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        print(\"Starting fitting for replication\", rep)\n",
    "\n",
    "        # Load data.\n",
    "        data_path = \"data/simulations/mhrm/\"\n",
    "        cell_path = data_path + \"sim_cell_\" + str(N_idx) + \"/\"\n",
    "        data = np.loadtxt(cell_path + \"data_\" + str(rep) + \".gz\")\n",
    "\n",
    "        # Make data loader.\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            tensor_dataset(torch.from_numpy(data)),\n",
    "            batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep * 100)\n",
    "        np.random.seed(rep * 100)\n",
    "\n",
    "        # Initialize model.\n",
    "        start = timeit.default_timer()\n",
    "        vae = MIRTVAEClass(input_dim = 500,\n",
    "                           inference_model_dims = [255],\n",
    "                           latent_dim = 10,\n",
    "                           n_cats = n_cats,\n",
    "                           learning_rate = 2.5e-3,\n",
    "                           device = device,\n",
    "                           Q = torch.from_numpy(block_diag(*([np.ones((10, 1))] * 10))).to(device).float(),\n",
    "                           correlated_factors = [0, 1, 2, 3, 4],\n",
    "                           steps_anneal = 1000)\n",
    "\n",
    "        # Fit model.\n",
    "        vae.run_training(data_loader, data_loader, iw_samples = 5)\n",
    "        stop = timeit.default_timer()\n",
    "        run_time = stop - start\n",
    "        print(\"Fitting completed in\", round(run_time, 2), \"seconds\")\n",
    "\n",
    "        # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "        loadings = vae.model.loadings.weight.data\n",
    "        intercepts = vae.model.intercepts.bias.data\n",
    "        scale_tril = vae.model.cholesky.weight().data\n",
    "        \n",
    "        # Compute approximate log-likelihood.\n",
    "        print(\"\\nComputing approx. LL for replication\", rep)\n",
    "        start = timeit.default_timer()\n",
    "        approx_ll = vae.bic(data_loader,\n",
    "                                iw_samples = 100)[1]\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"Approx. LL computed in\", round(stop - start, 2), \"seconds\")\n",
    "\n",
    "        # Make simulation cell directory.\n",
    "        res_path = \"results/simulations/mhrm/iwave/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Save extracted results.\n",
    "        Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   loadings.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "                   intercepts.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "                   scale_tril.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "                   np.asarray([approx_ll]),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"run_time/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"run_time/run_time_\" + str(rep) + \".txt\",\n",
    "                   np.asarray([run_time]),\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "\"\"\"\n",
    "NOTE: I manually refit replication 59 for sim. cell 1 and replication 89 for sim. cell 2\n",
    "due to convergence to poor local minima.\n",
    "\"\"\"\n",
    "        \n",
    "# Conduct MH-RM analyses.\n",
    "subprocess.call(\"code/r/mhrm_simulations.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make parameter estimate MSE boxplots and fitting time plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "data_path = \"data/simulations/\"\n",
    "gen_loadings = np.loadtxt(data_path + \"gen_params/five-factor/gen_loadings.txt\")\n",
    "gen_intercepts = np.loadtxt(data_path + \"gen_params/five-factor/gen_intercepts.txt\")\n",
    "gen_scale_tril = np.loadtxt(data_path + \"gen_params/five-factor/gen_scale_tril.txt\")\n",
    "\n",
    "# Modify data generating parameters.\n",
    "gen_loadings, gen_intercepts, gen_scale_tril = make_gen_params(orig_loadings = gen_loadings,\n",
    "                                                               orig_intercepts = gen_intercepts,\n",
    "                                                               orig_n_cats = 5,\n",
    "                                                               new_n_cats = 5,\n",
    "                                                               orig_cov = gen_scale_tril,\n",
    "                                                               factor_mul = 2)\n",
    "\n",
    "sample_size_ls = [1000, 2500, 5000, 10000]\n",
    "n_cats = [5] * 100\n",
    "n_reps = 100\n",
    "\n",
    "# Make list to store I-WAVE results.\n",
    "res_path = \"results/simulations/mhrm/iwave/\"\n",
    "iwave_cell_res_ls = []\n",
    "\n",
    "# Read in I-WAVE results.\n",
    "for sim_cell in range(4):\n",
    "    keys = [\"approx_ll\", \"run_time\", \"loadings\", \"intercepts\", \"cor\"]\n",
    "    cell_res = {key : [] for key in keys}\n",
    "    \n",
    "    # Read results.\n",
    "    cell_res[\"loadings\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/loadings/loadings_\" +\n",
    "                                        str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "    cell_res[\"intercepts\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/intercepts/intercepts_\" +\n",
    "                                         str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "    scale_tril_ls = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/scale_tril/scale_tril_\" +\n",
    "                                str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "    cell_res[\"cor\"] = [np.matmul(scale_tril, scale_tril.T) for scale_tril in scale_tril_ls]\n",
    "    cell_res[\"approx_ll\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/approx_ll/approx_ll_\" +\n",
    "                                        str(rep) + \".txt\", dtype = float).item() for rep in range(n_reps)]\n",
    "    cell_res[\"run_time\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/run_time/run_time_\" +\n",
    "                                       str(rep) + \".txt\", dtype = float).item() for rep in range(n_reps)]\n",
    "    \n",
    "    iwave_cell_res_ls.append(cell_res)\n",
    "\n",
    "# Make list to store MH-RM results.\n",
    "res_path = \"results/simulations/mhrm/mhrm/\"\n",
    "mhrm_cell_res_ls = []\n",
    "\n",
    "# Read in MH-RM results.\n",
    "for sim_cell in range(4):\n",
    "    keys = [\"ll\", \"run_time\", \"loadings\", \"intercepts\", \"cor\"]\n",
    "    cell_res = {key : [] for key in keys}\n",
    "    \n",
    "    # Read results.\n",
    "    cell_res[\"loadings\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/loadings/rep_\" +\n",
    "                                       str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "    cell_res[\"intercepts\"] = [-np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/intercepts/rep_\" +\n",
    "                                          str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "    cell_res[\"cor\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/cor/rep_\" +\n",
    "                                  str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "    cell_res[\"ll\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/ll/rep_\" +\n",
    "                                  str(rep) + \".txt\", dtype = float).item() for rep in range(n_reps)]\n",
    "    run_times = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/run_time/rep_\" +\n",
    "                            str(rep) + \".txt\", skiprows = 1, dtype = float) for rep in range(n_reps)]\n",
    "    cell_res[\"run_time\"] = [sum([run_time[i] for i in (2, 3)]) for run_time in run_times]\n",
    "    \n",
    "    mhrm_cell_res_ls.append(cell_res)\n",
    "\n",
    "# mirt does not report an intercept if a response category does not appear in the data.\n",
    "# Here, I identify runs where certain response categories were missing and insert NaNs\n",
    "# where appropriate.\n",
    "problem_reps = [idx for idx, ints in enumerate(mhrm_cell_res_ls[0][\"intercepts\"]) if\n",
    "                ints.shape[0] != 400]\n",
    "if len(problem_reps) != 0:\n",
    "    for rep in problem_reps:\n",
    "        # Read in data.\n",
    "        data = pd.read_csv(\"data/simulations/mhrm/sim_cell_0/data_\" + str(rep) + \".gz\",\n",
    "                           header = None, sep = \" \")\n",
    "        unique_vals = [data.iloc[:, col].unique() for col in data]\n",
    "        if any([len(vals) != 2 for vals in unique_vals]):\n",
    "            idxs = [len(vals) != 2 for vals in unique_vals].index(True)\n",
    "            ints = mhrm_cell_res_ls[0][\"intercepts\"][rep].copy()\n",
    "            temp_n_cats = [1] + [5]*100\n",
    "            for idx in [idxs]:\n",
    "                temp_ls = ints.tolist()\n",
    "                temp_ls.insert(np.cumsum([n_cat - 1 for n_cat in temp_n_cats])[int(np.floor(idx / 5))], np.nan)\n",
    "                ints = np.array(temp_ls)\n",
    "            np.savetxt(res_path + \"sim_cell_0/intercepts/rep_\" + str(rep) + \".txt\",\n",
    "                       -ints,\n",
    "                       fmt = \"%s\")\n",
    "    \n",
    "    # Read in MH-RM results, again.\n",
    "    mhrm_cell_res_ls = []\n",
    "    for sim_cell in range(4):\n",
    "        keys = [\"ll\", \"run_time\", \"loadings\", \"intercepts\", \"cor\"]\n",
    "        cell_res = {key : [] for key in keys}\n",
    "\n",
    "        # Read results.\n",
    "        cell_res[\"loadings\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/loadings/rep_\" +\n",
    "                                           str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "        cell_res[\"intercepts\"] = [-np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/intercepts/rep_\" +\n",
    "                                              str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "        cell_res[\"cor\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/cor/rep_\" +\n",
    "                                      str(rep) + \".txt\", dtype = float) for rep in range(n_reps)]\n",
    "        cell_res[\"ll\"] = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/ll/rep_\" +\n",
    "                                      str(rep) + \".txt\", dtype = float).item() for rep in range(n_reps)]\n",
    "        run_times = [np.loadtxt(res_path + \"sim_cell_\" + str(sim_cell) + \"/run_time/rep_\" +\n",
    "                                str(rep) + \".txt\", skiprows = 1, dtype = float) for rep in range(n_reps)]\n",
    "        cell_res[\"run_time\"] = [sum([run_time[i] for i in (2, 3)]) for run_time in run_times]\n",
    "\n",
    "        mhrm_cell_res_ls.append(cell_res)\n",
    "    \n",
    "mse = mhrm_boxplots(iwave_cell_res_ls = iwave_cell_res_ls,\n",
    "                    mhrm_cell_res_ls = mhrm_cell_res_ls,\n",
    "                    gen_cor = np.matmul(gen_scale_tril, gen_scale_tril.T),\n",
    "                    gen_loadings = gen_loadings,\n",
    "                    gen_intercepts = gen_intercepts,\n",
    "                    sample_size_ls = sample_size_ls)\n",
    "mse.show()\n",
    "\n",
    "times = comparison_time_plots(iwave_cell_res_ls,\n",
    "                              mhrm_cell_res_ls,\n",
    "                              sample_size_ls,\n",
    "                              lab1 = \"I-WAVE\", lab2 = \"MH-RM\",\n",
    "                              y_lims = [0, 1200])\n",
    "times.show()\n",
    "\n",
    "# Save plots to a single PDF.\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"figures/mse_plots_mhrm.pdf\")\n",
    "pdf.savefig(mse, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"figures/time_plot_mhrm.pdf\")\n",
    "pdf.savefig(times, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Two-Sample Test Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Empirical Type I Error and Power for Approximate C2STs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct approximate C2STs with uniformly distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr_types = [\"t1_error\", \"power\"]\n",
    "rr_type_names = [\"Type I error\", \"Power\"]\n",
    "sample_size_ls = [250, 500, 1000, 2500, 5000, 10000]\n",
    "n_reps = 100\n",
    "eps = 0.025\n",
    "nn_param_grid = {\n",
    "    \"alpha\" : np.logspace(-1, 1, 5),\n",
    "}\n",
    "\n",
    "# Conduct simulations.\n",
    "for rr_idx, rr_type in enumerate(rr_types):\n",
    "    print(\"\\n\" + rr_type_names[rr_idx] + \" verification\")\n",
    "\n",
    "    if rr_type == \"t1_error\":\n",
    "        real_dist = dist.Uniform(0., 1.)\n",
    "        synth_dist = dist.Uniform(0.05, 1.05)\n",
    "    else:\n",
    "        real_dist = dist.Uniform(0., 1.)\n",
    "        synth_dist = dist.Uniform(0.1, 1.1)\n",
    "\n",
    "    for N_idx, N in enumerate(sample_size_ls):\n",
    "        print(\"\\nStarting replications for N =\", N)\n",
    "\n",
    "        for rep in range(n_reps):\n",
    "            print(\"Starting C2STs for replication\", rep)\n",
    "\n",
    "            # Set random seeds.\n",
    "            torch.manual_seed(rep)\n",
    "            np.random.seed(rep)\n",
    "\n",
    "            # Simulate \"real\" data.\n",
    "            real_data = real_dist.sample([N]).unsqueeze(1)\n",
    "\n",
    "            # Simulate \"synthetic\" data.\n",
    "            synth_data = synth_dist.sample([N]).unsqueeze(1)\n",
    "\n",
    "            # Create combined real and synthetic data set.\n",
    "            X = torch.cat([real_data, synth_data], dim = 0).numpy()\n",
    "            y = torch.cat([torch.ones(N), torch.zeros(N)]).numpy()\n",
    "\n",
    "            # Conduct C2STs.\n",
    "            knn_res = c2st(X,\n",
    "                           y,\n",
    "                           neighbors.KNeighborsClassifier(n_neighbors = np.int(np.floor(np.sqrt(N))),\n",
    "                                                          metric = \"euclidean\",\n",
    "                                                          algorithm = \"ball_tree\"),\n",
    "                           eps = eps)\n",
    "            nn_res = c2st(X,\n",
    "                          y,\n",
    "                          neural_network.MLPClassifier(max_iter = np.int(np.floor(10000 / (N / 200))),\n",
    "                                                       random_state = rep * (2 * (rr_type == \"t1_error\"))),\n",
    "                          param_grid = nn_param_grid,\n",
    "                          eps = eps,\n",
    "                          random_state = rep * (2 * (rr_type == \"t1_error\")))\n",
    "\n",
    "            # Make directory to save results.\n",
    "            res_path = \"results/simulations/c2st/rr/\" + rr_type + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "            Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "            # Save results.\n",
    "            Path(res_path + \"knn_acc/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"knn_acc/knn_acc_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([knn_res[\"acc\"]]),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"knn_p_val/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"knn_p_val/knn_p_val_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([knn_res[\"p_val\"]]),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"nn_acc/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"nn_acc/nn_acc_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([nn_res[\"acc\"]]),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"nn_p_val/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"nn_p_val/nn_p_val_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([nn_res[\"p_val\"]]),\n",
    "                       fmt = \"%f\")\n",
    "\n",
    "plot_line_ls = [True, False]\n",
    "rr_lim_ls = [[0, 0.2], [-0.02, 1.02]]\n",
    "acc_lim_ls = [[0.46, 0.56], [0.49, 0.6]]\n",
    "acc_line_loc_ls = [0.525, 0.55]\n",
    "guide_p_val_ls_ls = [None, [approx_power(N = N, eps = .025, delta = .025, alpha = .05) for N in sample_size_ls]]\n",
    "for rr_idx, rr_type in enumerate(rr_types):\n",
    "    # Load accuracies and p-values.\n",
    "    knn_acc_ls_ls = []\n",
    "    knn_p_val_ls_ls = []\n",
    "    nn_acc_ls_ls = []\n",
    "    nn_p_val_ls_ls = []\n",
    "    for N_idx in range(len(sample_size_ls)):\n",
    "        res_path = \"results/simulations/c2st/rr/\" + rr_type + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        knn_acc_ls_ls.append([np.loadtxt(res_path + \"knn_acc/knn_acc_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                              rep in range(n_reps)])\n",
    "        knn_p_val_ls_ls.append([np.loadtxt(res_path + \"knn_p_val/knn_p_val_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                                rep in range(n_reps)])\n",
    "        nn_acc_ls_ls.append([np.loadtxt(res_path + \"nn_acc/nn_acc_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                             rep in range(n_reps)])\n",
    "        nn_p_val_ls_ls.append([np.loadtxt(res_path + \"nn_p_val/nn_p_val_\" + str(rep) + \".txt\", dtype = float).item() for\n",
    "                               rep in range(n_reps)])\n",
    "\n",
    "    # Make directory to save figures.\n",
    "    fig_path = \"figures/\"\n",
    "    Path(fig_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Create and save rejection rate plots for approximate C2STs.\n",
    "    fig = rr_acc_plot(knn_p_val_ls_ls = knn_p_val_ls_ls,\n",
    "                      nn_p_val_ls_ls = nn_p_val_ls_ls,\n",
    "                      knn_acc_ls_ls = knn_acc_ls_ls,\n",
    "                      nn_acc_ls_ls = nn_acc_ls_ls,\n",
    "                      sample_size_ls = sample_size_ls,\n",
    "                      guide_p_val_ls = guide_p_val_ls_ls[rr_idx],\n",
    "                      plot_line = plot_line_ls[rr_idx],\n",
    "                      rr_lim = rr_lim_ls[rr_idx],\n",
    "                      acc_lim = acc_lim_ls[rr_idx],\n",
    "                      rr_trans = True,\n",
    "                      acc_trans = True,\n",
    "                      acc_line_loc = acc_line_loc_ls[rr_idx])\n",
    "    fig.show()\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + rr_type + \".pdf\")\n",
    "    pdf.savefig(fig, dpi = 300)\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact and Approximate C2STs and C2ST-RFIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate five- and seven-factor MIRT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "data_path = \"data/simulations/\"\n",
    "ff_loadings = torch.from_numpy(np.loadtxt(data_path + \"gen_params/five-factor/gen_loadings.txt\")).float()\n",
    "ff_intercepts = torch.from_numpy(np.loadtxt(data_path + \"gen_params/five-factor/gen_intercepts.txt\")).float()\n",
    "ff_scale_tril = torch.from_numpy(np.loadtxt(data_path + \"gen_params/five-factor/gen_scale_tril.txt\")).float()\n",
    "sf_loadings = torch.from_numpy(np.loadtxt(data_path + \"gen_params/seven-factor/gen_loadings.txt\")).float()\n",
    "sf_intercepts = torch.from_numpy(np.loadtxt(data_path + \"gen_params/seven-factor/gen_intercepts.txt\")).float()\n",
    "sf_scale_tril = torch.from_numpy(np.loadtxt(data_path + \"gen_params/seven-factor/gen_scale_tril.txt\")).float()\n",
    "\n",
    "# Set some values for simulating data.\n",
    "sample_size_ls = [750, 1250, 2500, 5000, 10000]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "\n",
    "# Conduct simulations.\n",
    "for N_idx, N in enumerate(sample_size_ls):  \n",
    "    for rep in range(n_reps):\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep)\n",
    "        np.random.seed(rep)\n",
    "\n",
    "        # Simulate five-factor data.\n",
    "        sim_dist = dist.MultivariateNormal(loc = torch.zeros(5),\n",
    "                                           scale_tril = ff_scale_tril)\n",
    "        sim_data = sim_mirt(n_obs = N,\n",
    "                            distribution = sim_dist,\n",
    "                            loadings = ff_loadings,\n",
    "                            intercepts = ff_intercepts,\n",
    "                            n_cats = n_cats)[0].numpy()\n",
    "        \n",
    "        # Save five-factor data set.\n",
    "        cell_path = data_path + \"c2st/seven-factor/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(cell_path).mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(cell_path + \"data_\" + str(rep) + \".gz\",\n",
    "                   sim_data,\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "        # Set random seeds again.\n",
    "        torch.manual_seed(rep)\n",
    "        np.random.seed(rep)\n",
    "        \n",
    "        # Simulate seven-factor data.\n",
    "        sim_dist = dist.MultivariateNormal(loc = torch.zeros(7),\n",
    "                                           scale_tril = sf_scale_tril)\n",
    "        sim_data = sim_mirt(n_obs = N,\n",
    "                            distribution = sim_dist,\n",
    "                            loadings = sf_loadings,\n",
    "                            intercepts = sf_intercepts,\n",
    "                            n_cats = n_cats)[0].numpy()\n",
    "        \n",
    "        # Save seven-factor data set.\n",
    "        cell_path = data_path + \"c2st/seven-factor/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(cell_path).mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(cell_path + \"data_\" + str(rep) + \".gz\",\n",
    "                   sim_data,\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "print(\"Finished simulating data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit crossing of data generating and fitted models and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_ls = [750, 1250, 2500, 5000, 10000]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "\n",
    "# Make loadings constraints matrices.\n",
    "Q = torch.from_numpy(block_diag(*([np.ones((10, 1))] * 5))).to(device).float()\n",
    "A = torch.from_numpy(block_diag(*[np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.eye(10), np.zeros([50, 50]),\n",
    "                                  np.zeros([50, 50])])).to(device).float()\n",
    "A[266, 266] += 1; A[267, 266] += 1; A[340, 340] += 1; A[347, 340] += 1\n",
    "\n",
    "# Loop through simulation cells and replications.\n",
    "for N_idx, N in enumerate(sample_size_ls):\n",
    "    print(\"\\nStarting replications for N =\", N)\n",
    "\n",
    "    for rep in range(n_reps):\n",
    "        print(\"Starting fitting for replication\", rep)\n",
    "\n",
    "        # Load five-factor data.\n",
    "        ff_data_path = \"data/simulations/c2st/five-factor/\"\n",
    "        ff_cell_path = ff_data_path + \"sim_cell_\" + str(N_idx) + \"/\"\n",
    "        ff_data = np.loadtxt(ff_cell_path + \"data_\" + str(rep) + \".gz\")\n",
    "\n",
    "        # Make data loader.\n",
    "        ff_data_loader = torch.utils.data.DataLoader(\n",
    "            tensor_dataset(torch.from_numpy(ff_data)),\n",
    "            batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep * 100)\n",
    "        np.random.seed(rep * 100)\n",
    "\n",
    "        # Fit five-factor model.\n",
    "        ff_vae = MIRTVAEClass(input_dim = 250,\n",
    "                              inference_model_dims = [130],\n",
    "                              latent_dim = 5,\n",
    "                              n_cats = n_cats,\n",
    "                              learning_rate = 5e-3,\n",
    "                              device = device,\n",
    "                              Q = Q,\n",
    "                              correlated_factors = [0, 1, 2, 3, 4],\n",
    "                              steps_anneal = 1000)\n",
    "        ff_vae.run_training(ff_data_loader,\n",
    "                            ff_data_loader,\n",
    "                            iw_samples = 5)\n",
    "\n",
    "        # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "        ff_loadings = ff_vae.model.loadings.weight.data\n",
    "        ff_intercepts = ff_vae.model.intercepts.bias.data\n",
    "        ff_scale_tril = ff_vae.model.cholesky.weight().data\n",
    "\n",
    "        # Compute approximate log-likelihood.\n",
    "        ff_approx_ll = ff_vae.bic(ff_data_loader,\n",
    "                                  iw_samples = 100)[1]\n",
    "\n",
    "        # Make simulation cell directory.\n",
    "        res_path = \"results/simulations/c2st/dg_five_fitted_five/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Save extracted results.\n",
    "        Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   ff_loadings.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "                   ff_intercepts.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "                   ff_scale_tril.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "                   np.asarray([ff_approx_ll]),\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "        # Re-set random seeds.\n",
    "        torch.manual_seed(rep * 100)\n",
    "        np.random.seed(rep * 100)\n",
    "\n",
    "        # Fit seven-factor model.\n",
    "        sf_vae = MIRTVAEClass(input_dim = 250,\n",
    "                              inference_model_dims = [130],\n",
    "                              latent_dim = 7,\n",
    "                              n_cats = n_cats,\n",
    "                              learning_rate = 5e-3,\n",
    "                              device = device,\n",
    "                              A = A,\n",
    "                              correlated_factors = [0, 1, 2, 3, 4],\n",
    "                              steps_anneal = 1000)\n",
    "        sf_vae.run_training(ff_data_loader,\n",
    "                            ff_data_loader,\n",
    "                            iw_samples = 5)\n",
    "\n",
    "        # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "        sf_loadings = sf_vae.model.loadings.weight().data\n",
    "        sf_intercepts = sf_vae.model.intercepts.bias.data\n",
    "        sf_scale_tril = sf_vae.model.cholesky.weight().data\n",
    "\n",
    "        # Compute approximate log-likelihood.\n",
    "        sf_approx_ll = sf_vae.bic(ff_data_loader,\n",
    "                                  iw_samples = 100)[1]\n",
    "\n",
    "        # Make simulation cell directory.\n",
    "        res_path = \"results/simulations/c2st/dg_five_fitted_seven/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Save extracted results.\n",
    "        Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   sf_loadings.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "                   sf_intercepts.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "                   sf_scale_tril.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "                   np.asarray([sf_approx_ll]),\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        # Load seven-factor data.\n",
    "        sf_data_path = \"data/simulations/c2st/seven-factor/\"\n",
    "        sf_cell_path = sf_data_path + \"sim_cell_\" + str(N_idx) + \"/\"\n",
    "        sf_data = np.loadtxt(sf_cell_path + \"data_\" + str(rep) + \".gz\")\n",
    "\n",
    "        # Make data loader.\n",
    "        sf_data_loader = torch.utils.data.DataLoader(\n",
    "            tensor_dataset(torch.from_numpy(sf_data)),\n",
    "            batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "        # Re-set random seeds.\n",
    "        torch.manual_seed(rep * 100)\n",
    "        np.random.seed(rep * 100)\n",
    "\n",
    "        # Fit five-factor model.\n",
    "        ff_vae = MIRTVAEClass(input_dim = 250,\n",
    "                              inference_model_dims = [130],\n",
    "                              latent_dim = 5,\n",
    "                              n_cats = n_cats,\n",
    "                              learning_rate = 5e-3,\n",
    "                              device = device,\n",
    "                              Q = Q,\n",
    "                              correlated_factors = [0, 1, 2, 3, 4],\n",
    "                              steps_anneal = 1000)\n",
    "        ff_vae.run_training(sf_data_loader,\n",
    "                            sf_data_loader,\n",
    "                            iw_samples = 5)\n",
    "\n",
    "        # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "        ff_loadings = ff_vae.model.loadings.weight.data\n",
    "        ff_intercepts = ff_vae.model.intercepts.bias.data\n",
    "        ff_scale_tril = ff_vae.model.cholesky.weight().data\n",
    "\n",
    "        # Compute approximate log-likelihood.\n",
    "        ff_approx_ll = ff_vae.bic(sf_data_loader,\n",
    "                                  iw_samples = 100)[1]\n",
    "\n",
    "        # Make simulation cell directory.\n",
    "        res_path = \"results/simulations/c2st/dg_seven_fitted_five/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Save extracted results.\n",
    "        Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   ff_loadings.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "                   ff_intercepts.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "                   ff_scale_tril.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "                   np.asarray([ff_approx_ll]),\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "        # Re-set random seeds.\n",
    "        torch.manual_seed(rep * 100)\n",
    "        np.random.seed(rep * 100)\n",
    "\n",
    "        # Fit seven-factor model.\n",
    "        sf_vae = MIRTVAEClass(input_dim = 250,\n",
    "                              inference_model_dims = [130],\n",
    "                              latent_dim = 7,\n",
    "                              n_cats = n_cats,\n",
    "                              learning_rate = 5e-3,\n",
    "                              device = device,\n",
    "                              A = A,\n",
    "                              correlated_factors = [0, 1, 2, 3, 4],\n",
    "                              steps_anneal = 1000)\n",
    "        sf_vae.run_training(sf_data_loader,\n",
    "                            sf_data_loader,\n",
    "                            iw_samples = 5)\n",
    "\n",
    "        # Extract estimated loadings, intercepts, and factor correlation matrix Cholesky decomposition.\n",
    "        sf_loadings = sf_vae.model.loadings.weight().data\n",
    "        sf_intercepts = sf_vae.model.intercepts.bias.data\n",
    "        sf_scale_tril = sf_vae.model.cholesky.weight().data\n",
    "\n",
    "        # Compute approximate log-likelihood.\n",
    "        sf_approx_ll = sf_vae.bic(sf_data_loader,\n",
    "                                  iw_samples = 100)[1]\n",
    "\n",
    "        # Make simulation cell directory.\n",
    "        res_path = \"results/simulations/c2st/dg_seven_fitted_seven/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Save extracted results.\n",
    "        Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   sf_loadings.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "                   sf_intercepts.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"scale_tril/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\",\n",
    "                   sf_scale_tril.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "        Path(res_path + \"approx_ll/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"approx_ll/approx_ll_\" + str(rep) + \".txt\",\n",
    "                   np.asarray([sf_approx_ll]),\n",
    "                   fmt = \"%f\")\n",
    "        \n",
    "\"\"\"\n",
    "Needed to manually refit the following runs due to convergence to poor local minima:\n",
    "    DG = five-factor, fitted = five-factor:\n",
    "        N = 1250:  6, 9, 34, 41, 43, 61, 64, 72, 85, 94, 98\n",
    "    DG = five-factor, fitted = seven-factor:\n",
    "        N = 750: 50\n",
    "        N = 1250: 10, 31, 45, 50, 72, 86, 87, 97\n",
    "    DG = seven-factor, fitted = five-factor:\n",
    "        N = 750: 18, [33], 35, 43, 59, 80, [90], 98\n",
    "        N = 1250: 22, [31], 49, 84, 90\n",
    "        N = 2500: 95\n",
    "    DG = seven-factor, fitted = seven-factor:\n",
    "        N = 750: 2\n",
    "        N = 1250: 65, 67, 71, 76, 82, 93\n",
    "Bad runs were identified via their outlying approx. LLs.\n",
    "Used seed = rep * 1000, then seed = rep * 2000 for runs in brackets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct exact and approximate C2STs for each crossing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossings = [\"dg_five_fitted_five\", \"dg_five_fitted_seven\",\n",
    "             \"dg_seven_fitted_five\", \"dg_seven_fitted_seven\"]\n",
    "crossing_names = [(\"Five\", \"Five\"), (\"Five\", \"Seven\"),\n",
    "                  (\"Seven\", \"Five\"), (\"Seven\", \"Seven\")]\n",
    "sample_size_ls = [750, 1250, 2500, 5000, 10000]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "eps = 0.025\n",
    "nn_param_grid = {\n",
    "    \"alpha\" : np.logspace(-1, 1, 5),\n",
    "}\n",
    "\n",
    "for crossing_idx, crossing in enumerate(crossings):\n",
    "    print(\"\\nDG = \" + crossing_names[crossing_idx][0] + \", Fitted = \" + crossing_names[crossing_idx][1])\n",
    "    \n",
    "    for N_idx, N in enumerate(sample_size_ls):\n",
    "        print(\"Starting replications for N =\", N)\n",
    "\n",
    "        for rep in range(n_reps):\n",
    "            print(\"Starting C2STs for replication\", rep)\n",
    "\n",
    "            # Set random seeds.\n",
    "            torch.manual_seed(rep)\n",
    "            np.random.seed(rep)\n",
    "\n",
    "            # Load \"real\" data.\n",
    "            if \"dg_five\" in crossing:\n",
    "                data_path = \"data/simulations/c2st/five-factor/\"\n",
    "            else:\n",
    "                data_path = \"data/simulations/c2st/seven-factor/\"\n",
    "            cell_path = data_path + \"sim_cell_\" + str(N_idx) + \"/\"\n",
    "            real_data = np.loadtxt(cell_path + \"data_\" + str(rep) + \".gz\")\n",
    "\n",
    "            # Integer encode real data.\n",
    "            idxs = np.concatenate((np.zeros(1), np.cumsum(n_cats)))\n",
    "            ranges = [np.arange(int(l), int(u)) for l, u in zip(idxs, idxs[1:])]\n",
    "            real_data_int = np.concatenate([np.expand_dims(np.argmax(real_data[:, rng], axis = 1), axis = 1) for\n",
    "                                            rng in ranges], axis = 1)\n",
    "\n",
    "            # Load estimated parameters from correct models.\n",
    "            res_path = \"results/simulations/c2st/\" + crossing + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "            fitted_loadings = np.loadtxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\")\n",
    "            fitted_intercepts = np.loadtxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\")\n",
    "            fitted_scale_tril = np.loadtxt(res_path + \"scale_tril/scale_tril_\" + str(rep) + \".txt\")\n",
    "            \n",
    "            # List to store run times.\n",
    "            time_ls = []\n",
    "\n",
    "            # Simulate \"synthetic\" data.\n",
    "            if \"fitted_five\" in crossing:\n",
    "                latent_dim = 5\n",
    "            else:\n",
    "                latent_dim = 7\n",
    "            start = timeit.default_timer()\n",
    "            synth_dist = dist.MultivariateNormal(loc = torch.zeros(latent_dim),\n",
    "                                                 scale_tril = torch.from_numpy(fitted_scale_tril).float())\n",
    "            synth_data = sim_mirt(n_obs = N,\n",
    "                                  distribution = synth_dist,\n",
    "                                  loadings = torch.from_numpy(fitted_loadings).float(),\n",
    "                                  intercepts = torch.from_numpy(fitted_intercepts).float(),\n",
    "                                  n_cats = n_cats,\n",
    "                                  dummy_code = False)[0]\n",
    "            stop = timeit.default_timer()\n",
    "            time_ls.append(stop - start)\n",
    "\n",
    "            # Create combined real and synthetic data set.\n",
    "            X = torch.cat([torch.from_numpy(real_data_int), synth_data], dim = 0).numpy()\n",
    "            y = torch.cat([torch.ones(N), torch.zeros(N)]).numpy()\n",
    "\n",
    "            # Conduct C2STs.\n",
    "            start = timeit.default_timer()\n",
    "            knn_res = c2st(X,\n",
    "                           y,\n",
    "                           neighbors.KNeighborsClassifier(n_neighbors = np.int(np.floor(np.sqrt(N))),\n",
    "                                                          metric = \"hamming\",\n",
    "                                                          algorithm = \"ball_tree\"),\n",
    "                           eps = eps,\n",
    "                           random_state = rep)\n",
    "            stop = timeit.default_timer()\n",
    "            time_ls.append(stop - start)\n",
    "            start = timeit.default_timer()\n",
    "            nn_res = c2st(X,\n",
    "                          y,\n",
    "                          neural_network.MLPClassifier(max_iter = np.int(np.floor(10000 / (N / 200))),\n",
    "                                                       random_state = rep),\n",
    "                          param_grid = nn_param_grid,\n",
    "                          eps = eps,\n",
    "                          random_state = rep)\n",
    "            stop = timeit.default_timer()\n",
    "            y_pred = nn_res[\"grid_clf\"]. best_estimator_.predict(nn_res[\"X_test\"])\n",
    "            time_ls.append(stop - start)\n",
    "            \n",
    "            # Obtain p-values for exact C2STs.\n",
    "            knn_exact_p_val = 1 - norm(loc = 0.5, scale = np.sqrt(0.25 / N)).cdf(knn_res[\"acc\"])\n",
    "            nn_exact_p_val = 1 - norm(loc = 0.5, scale = np.sqrt(0.25 / N)).cdf(nn_res[\"acc\"])\n",
    "            \n",
    "            # Compute permutation importances.\n",
    "            if N == 10000 and crossing == \"dg_seven_fitted_five\":\n",
    "                _, X_sub, _, y_sub = train_test_split(knn_res[\"X_test\"], knn_res[\"y_test\"], test_size = 0.05)\n",
    "                start = timeit.default_timer()\n",
    "                knn_imp = permutation_importance(knn_res[\"clf\"],\n",
    "                                                 knn_res[\"X_test\"],\n",
    "                                                 knn_res[\"y_test\"], \n",
    "                                                 scoring = \"accuracy\", \n",
    "                                                 random_state = rep)\n",
    "                stop = timeit.default_timer()\n",
    "                time_ls.append(stop - start)\n",
    "                start = timeit.default_timer()\n",
    "                nn_imp = permutation_importance(nn_res[\"grid_clf\"]. best_estimator_,\n",
    "                                                nn_res[\"X_test\"],\n",
    "                                                nn_res[\"y_test\"], \n",
    "                                                scoring = \"accuracy\", \n",
    "                                                random_state = rep)\n",
    "                stop = timeit.default_timer()\n",
    "                time_ls.append(stop - start)\n",
    "\n",
    "            # Save results.\n",
    "            res_path = \"results/simulations/c2st/\" + crossing + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "            Path(res_path + \"run_times/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"run_times/run_times_\" + str(rep) + \".txt\",\n",
    "                       np.asarray(time_ls),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"knn_res/\").mkdir(parents = True, exist_ok = True)\n",
    "            save_obj(knn_res,\n",
    "                     res_path + \"knn_res/knn_res_\" + str(rep))\n",
    "            Path(res_path + \"knn_exact_p_val/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"knn_exact_p_val/knn_exact_p_val_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([knn_exact_p_val]),\n",
    "                       fmt = \"%f\")\n",
    "            Path(res_path + \"nn_res/\").mkdir(parents = True, exist_ok = True)\n",
    "            save_obj(nn_res,\n",
    "                     res_path + \"nn_res/nn_res_\" + str(rep))\n",
    "            Path(res_path + \"nn_exact_p_val/\").mkdir(parents = True, exist_ok = True)\n",
    "            np.savetxt(res_path + \"nn_exact_p_val/nn_exact_p_val_\" + str(rep) + \".txt\",\n",
    "                       np.asarray([nn_exact_p_val]),\n",
    "                       fmt = \"%f\")\n",
    "            if N == 10000 and crossing == \"dg_seven_fitted_five\":\n",
    "                Path(res_path + \"knn_imp/\").mkdir(parents = True, exist_ok = True)\n",
    "                save_obj(knn_imp,\n",
    "                         res_path + \"knn_imp/knn_imp_\" + str(rep))\n",
    "                Path(res_path + \"nn_imp/\").mkdir(parents = True, exist_ok = True)\n",
    "                save_obj(nn_imp,\n",
    "                         res_path + \"nn_imp/nn_imp_\" + str(rep))\n",
    "\n",
    "plot_line_ls = [True, True, False, True]\n",
    "rr_lim_ls = [[-0.01, 0.2], [-0.01, 0.2], [-0.02, 1.02], [-0.01, 0.2]]\n",
    "acc_lim_ls = [[0.4, 0.56], [0.42, 0.56], [0.44, 0.65], [0.42, 0.56]]\n",
    "rr_legend_size_ls = [10, 10, 9, 10]\n",
    "for crossing_idx, crossing in enumerate(crossings):\n",
    "    # Load accuracies and p-values.\n",
    "    knn_acc_ls_ls = []\n",
    "    knn_approx_p_val_ls_ls = []\n",
    "    knn_exact_p_val_ls_ls = []\n",
    "    nn_acc_ls_ls = []\n",
    "    nn_approx_p_val_ls_ls = []\n",
    "    nn_exact_p_val_ls_ls = []\n",
    "    for N_idx in range(len(sample_size_ls)):\n",
    "        res_path = \"results/simulations/c2st/\" + crossing + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        knn_acc_ls_ls.append([load_obj(res_path + \"knn_res/knn_res_\" + str(rep))[\"acc\"] for\n",
    "                              rep in range(n_reps)])\n",
    "        knn_approx_p_val_ls_ls.append([load_obj(res_path + \"knn_res/knn_res_\" + str(rep))[\"p_val\"] for\n",
    "                                       rep in range(n_reps)])\n",
    "        knn_exact_p_val_ls_ls.append([np.loadtxt(res_path + \"knn_exact_p_val/knn_exact_p_val_\" + str(rep) + \".txt\",\n",
    "                                                 dtype = float).item() for rep in range(n_reps)])\n",
    "        nn_acc_ls_ls.append([load_obj(res_path + \"nn_res/nn_res_\" + str(rep))[\"acc\"] for\n",
    "                             rep in range(n_reps)])\n",
    "        nn_approx_p_val_ls_ls.append([load_obj(res_path + \"nn_res/nn_res_\" + str(rep))[\"p_val\"] for\n",
    "                                      rep in range(n_reps)])\n",
    "        nn_exact_p_val_ls_ls.append([np.loadtxt(res_path + \"nn_exact_p_val/nn_exact_p_val_\" + str(rep) + \".txt\",\n",
    "                                                dtype = float).item() for rep in range(n_reps)])\n",
    "\n",
    "    # Make directory to save figures.\n",
    "    fig_path = \"figures/\"\n",
    "    Path(fig_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Create and save rejection rate plots for C2STs.\n",
    "    fig = mul_rr_acc_plots(knn_p_val_res_ls = [knn_approx_p_val_ls_ls,\n",
    "                                              knn_exact_p_val_ls_ls],\n",
    "                           nn_p_val_res_ls = [nn_approx_p_val_ls_ls,\n",
    "                                              nn_exact_p_val_ls_ls],\n",
    "                           knn_acc_ls_ls = knn_acc_ls_ls,\n",
    "                           nn_acc_ls_ls = nn_acc_ls_ls,\n",
    "                           sample_size_ls = sample_size_ls,\n",
    "                           plot_line = plot_line_ls[crossing_idx],\n",
    "                           rr_lim = rr_lim_ls[crossing_idx],\n",
    "                           acc_lim = acc_lim_ls[crossing_idx],\n",
    "                           rr_legend_size = rr_legend_size_ls[crossing_idx],\n",
    "                           rr_trans = True,\n",
    "                           acc_trans = True)\n",
    "    fig.show()\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"rr_acc_\" + crossing + \".pdf\")\n",
    "    pdf.savefig(fig, dpi = 300)\n",
    "    pdf.close()\n",
    "\n",
    "res_path = \"results/simulations/c2st/dg_seven_fitted_five/sim_cell_\" + str(N_idx) + \"/\"\n",
    "knn_imp_ls = [load_obj(res_path + \"knn_imp/knn_imp_\" + str(rep)) for rep in range(n_reps)]\n",
    "nn_imp_ls = [load_obj(res_path + \"nn_imp/nn_imp_\" + str(rep)) for rep in range(n_reps)]\n",
    "fig = importance_plot(knn_imp_ls,\n",
    "                      nn_imp_ls,\n",
    "                      varnames = [\"Items 1–10\",\n",
    "                                  \"Items 11–20\",\n",
    "                                  \"Items 21–30\",\n",
    "                                  \"Items 31–40\",\n",
    "                                  \"Items 41–50\"],\n",
    "                      knn_title = \"KNN Classifiers\",\n",
    "                      nn_title = \"NN Classifiers\",\n",
    "                      knn_ylim = [-0.005, 0.085],\n",
    "                      nn_ylim = [-0.005, 0.085])\n",
    "fig.show()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"importances_dg_seven_fitted_five.pdf\")\n",
    "pdf.savefig(fig, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute C2ST-RFIs for each crossing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgs = [\"five-factor\", \"seven-factor\"]\n",
    "crossings = [\"dg_five_fitted_five\", \"dg_five_fitted_seven\",\n",
    "             \"dg_seven_fitted_five\", \"dg_seven_fitted_seven\"]\n",
    "sample_size_ls = [750, 1250, 2500, 5000, 10000]\n",
    "n_cats = [5] * 50\n",
    "n_reps = 100\n",
    "nn_param_grid = {\n",
    "    \"alpha\" : np.logspace(-1, 1, 5),\n",
    "}\n",
    "\n",
    "for dg in dgs:\n",
    "    for N_idx, N in enumerate(sample_size_ls):\n",
    "        print(\"\\nStarting replications for N =\", N)\n",
    "\n",
    "        for rep in range(n_reps):\n",
    "            print(\"Starting C2STs for replication\", rep)\n",
    "\n",
    "            # Set random seeds.\n",
    "            torch.manual_seed(rep)\n",
    "            np.random.seed(rep)\n",
    "\n",
    "            # Load \"real\" data.\n",
    "            data_path = \"data/simulations/c2st/\" + dg + \"/\"\n",
    "            cell_path = data_path + \"sim_cell_\" + str(N_idx) + \"/\"\n",
    "            real_data = np.loadtxt(cell_path + \"data_\" + str(rep) + \".gz\")\n",
    "\n",
    "            # Integer encode real data.\n",
    "            idxs = np.concatenate((np.zeros(1), np.cumsum(n_cats)))\n",
    "            ranges = [np.arange(int(l), int(u)) for l, u in zip(idxs, idxs[1:])]\n",
    "            real_data_int = np.concatenate([np.expand_dims(np.argmax(real_data[:, rng], axis = 1), axis = 1) for\n",
    "                                            rng in ranges], axis = 1)\n",
    "            \n",
    "            # List to store run times.\n",
    "            time_ls = []\n",
    "\n",
    "            # Simulate synthetic data from baseline model.\n",
    "            start = timeit.default_timer()\n",
    "            synth_data = sim_base(data = torch.from_numpy(real_data),\n",
    "                                  n_cats = n_cats,\n",
    "                                  dummy_code = False)\n",
    "            stop = timeit.default_timer()\n",
    "            time_ls.append(stop - start)\n",
    "\n",
    "            # Create combined real and synthetic (i.e., from baseline model) data set.\n",
    "            X_base = torch.cat([torch.from_numpy(real_data_int), synth_data], dim = 0).numpy()\n",
    "            y_base = torch.cat([torch.ones(N), torch.zeros(N)]).numpy()\n",
    "\n",
    "            # Conduct C2STs for null model.\n",
    "            start = timeit.default_timer()\n",
    "            knn_acc_base = c2st(X_base,\n",
    "                                y_base,\n",
    "                                neighbors.KNeighborsClassifier(n_neighbors = np.int(np.floor(np.sqrt(N))),\n",
    "                                                               metric = \"hamming\",\n",
    "                                                               algorithm = \"ball_tree\"),\n",
    "                                random_state = rep)[\"acc\"]\n",
    "            stop = timeit.default_timer()\n",
    "            time_ls.append(stop - start)\n",
    "            start = timeit.default_timer()\n",
    "            nn_acc_base = c2st(X_base,\n",
    "                               y_base,\n",
    "                               neural_network.MLPClassifier(max_iter = np.int(np.floor(10000 / (N / 200))),\n",
    "                                                            random_state = rep),\n",
    "                               param_grid = nn_param_grid,\n",
    "                               random_state = rep)[\"acc\"]\n",
    "            stop = timeit.default_timer()\n",
    "            time_ls.append(stop - start)\n",
    "\n",
    "            # Store results.\n",
    "            for crossing in [crsg for crsg in crossings if (\"dg_\" + dg[0:4]) in crsg]:\n",
    "                res_path = \"results/simulations/c2st/\" + crossing + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "                Path(res_path + \"base_run_times/\").mkdir(parents = True, exist_ok = True)\n",
    "                np.savetxt(res_path + \"base_run_times/base_run_times_\" + str(rep) + \".txt\",\n",
    "                       np.asarray(time_ls),\n",
    "                       fmt = \"%f\")\n",
    "                Path(res_path + \"nn_acc_base/\").mkdir(parents = True, exist_ok = True)\n",
    "                np.savetxt(res_path + \"nn_acc_base/nn_acc_base_\" + str(rep) + \".txt\",\n",
    "                           np.asarray([nn_acc_base]),\n",
    "                           fmt = \"%f\")\n",
    "                Path(res_path + \"knn_acc_base/\").mkdir(parents = True, exist_ok = True)\n",
    "                np.savetxt(res_path + \"knn_acc_base/knn_acc_base_\" + str(rep) + \".txt\",\n",
    "                           np.asarray([knn_acc_base]),\n",
    "                           fmt = \"%f\")\n",
    "\n",
    "for crossing in crossings:\n",
    "    # Load accuracies.\n",
    "    knn_acc_prop_ls_ls = []\n",
    "    knn_acc_base_ls_ls = []\n",
    "    nn_acc_prop_ls_ls = []\n",
    "    nn_acc_base_ls_ls = []\n",
    "    for N_idx in range(len(sample_size_ls)):\n",
    "        res_path = \"results/simulations/c2st/\" + crossing + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        knn_acc_prop_ls_ls.append([load_obj(res_path + \"knn_res/knn_res_\" + str(rep))[\"acc\"] for\n",
    "                                   rep in range(n_reps)])\n",
    "        knn_acc_base_ls_ls.append([np.loadtxt(res_path + \"knn_acc_base/knn_acc_base_\" + str(rep) + \".txt\",\n",
    "                                              dtype = float).item() for\n",
    "                                   rep in range(n_reps)])\n",
    "        nn_acc_prop_ls_ls.append([load_obj(res_path + \"nn_res/nn_res_\" + str(rep))[\"acc\"] for\n",
    "                                  rep in range(n_reps)])\n",
    "        nn_acc_base_ls_ls.append([np.loadtxt(res_path + \"nn_acc_base/nn_acc_base_\" + str(rep) + \".txt\",\n",
    "                                             dtype = float).item() for\n",
    "                                  rep in range(n_reps)])\n",
    "\n",
    "\n",
    "    # Compute relative fit indices.\n",
    "    M_prop = 265\n",
    "    M_base = 200\n",
    "    knn_rfi_ls_ls = [[c2st_rfi(acc_prop, acc_base, M_prop, M_base, lambda a : a) for\n",
    "                      acc_prop, acc_base in zip(acc_prop_ls, acc_base_ls)] for\n",
    "                     acc_prop_ls, acc_base_ls in zip(knn_acc_prop_ls_ls, knn_acc_base_ls_ls)]\n",
    "    nn_rfi_ls_ls = [[c2st_rfi(acc_prop, acc_base, M_prop, M_base, lambda a : a) for\n",
    "                     acc_prop, acc_base in zip(acc_prop_ls, acc_base_ls)] for\n",
    "                    acc_prop_ls, acc_base_ls in zip(nn_acc_prop_ls_ls, nn_acc_base_ls_ls)]\n",
    "\n",
    "    # Make directory to save figures.\n",
    "    fig_path = \"figures/\"\n",
    "    Path(fig_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Create and save figures.\n",
    "    fig = c2st_rfi_boxplot(knn_rfi_res = knn_rfi_ls_ls,\n",
    "                           nn_rfi_res = nn_rfi_ls_ls,\n",
    "                           sample_size_ls = sample_size_ls)\n",
    "    fig.show()\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"c2st-rfi_\" + crossing + \".pdf\")\n",
    "    pdf.savefig(fig, dpi = 300)\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make run time plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossings = [\"dg_five_fitted_five\", \"dg_five_fitted_seven\",\n",
    "             \"dg_seven_fitted_five\", \"dg_seven_fitted_seven\"]\n",
    "sample_size_ls = [750, 1250, 2500, 5000, 10000]\n",
    "n_reps = 100\n",
    "\n",
    "for crossing in crossings:\n",
    "    # Load raw run times.\n",
    "    run_times_ls_ls      = []\n",
    "    base_run_times_ls_ls = []\n",
    "    for N_idx in range(len(sample_size_ls)):\n",
    "        res_path = \"results/simulations/c2st/\" + crossing + \"/sim_cell_\" + str(N_idx) + \"/\"\n",
    "        run_times_ls_ls.append([np.loadtxt(res_path + \"run_times/run_times_\" + str(rep) + \".txt\",\n",
    "                                           dtype = float) for rep in range(n_reps)])\n",
    "        base_run_times_ls_ls.append([np.loadtxt(res_path + \"base_run_times/base_run_times_\" + str(rep) + \".txt\",\n",
    "                                                dtype = float) for rep in range(n_reps)])\n",
    "        \n",
    "        # Compute total run times.\n",
    "        knn_c2st_run_times_ls_ls = [[run_times[0:2].sum() for run_times in run_times_ls] for\n",
    "                                    run_times_ls in run_times_ls_ls]\n",
    "        nn_c2st_run_times_ls_ls = [[run_times[[0, 2]].sum() for run_times in run_times_ls] for\n",
    "                                   run_times_ls in run_times_ls_ls]\n",
    "        knn_rfi_run_times_ls_ls = [[run_times[0:2].sum() + base_run_times[0:2].sum() for\n",
    "                                    run_times, base_run_times in zip(run_times_ls, base_run_times_ls)] for\n",
    "                                   run_times_ls, base_run_times_ls in zip(run_times_ls_ls, base_run_times_ls_ls)]\n",
    "        nn_rfi_run_times_ls_ls = [[run_times[[0, 2]].sum() + base_run_times[[0, 2]].sum() for\n",
    "                                   run_times, base_run_times in zip(run_times_ls, base_run_times_ls)] for\n",
    "                                  run_times_ls, base_run_times_ls in zip(run_times_ls_ls, base_run_times_ls_ls)]\n",
    "\n",
    "    # Make directory to save figures.\n",
    "    fig_path = \"figures/\"\n",
    "    Path(fig_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Create and save figures.\n",
    "    c2st_fig = c2st_time_plot(run_times_ls_ls1 = knn_c2st_run_times_ls_ls,\n",
    "                              run_times_ls_ls2 = nn_c2st_run_times_ls_ls,\n",
    "                              sample_size_ls = sample_size_ls,\n",
    "                              lab1 = \"KNN\",\n",
    "                              lab2 = \"NN\")\n",
    "    c2st_fig.show()\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"time_plot_c2st_\" + crossing + \".pdf\")\n",
    "    pdf.savefig(c2st_fig, dpi = 300)\n",
    "    pdf.close()\n",
    "    \n",
    "    rfi_fig = c2st_time_plot(run_times_ls_ls1 = knn_rfi_run_times_ls_ls,\n",
    "                              run_times_ls_ls2 = nn_rfi_run_times_ls_ls,\n",
    "                              sample_size_ls = sample_size_ls,\n",
    "                              lab1 = \"KNN\",\n",
    "                              lab2 = \"NN\")\n",
    "    rfi_fig.show()\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(fig_path + \"time_plot_c2st-rfi_\" + crossing + \".pdf\")\n",
    "    pdf.savefig(rfi_fig, dpi = 300)\n",
    "    pdf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
